{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is just a copy of the `.py` file so it's easier to run remotely**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from fastcoref import FCoref\n",
    "import wiki_dump_reader as wiki\n",
    "from typing import Dict, Generator, List, Tuple\n",
    "\n",
    "def my_build_links(text: str) -> Tuple[str, List[Dict]]:\n",
    "    # We don't want to just remove the links, we want to replace them with the link text so that the returned text\n",
    "    # is parseable. We also want to make sure the links indices are consistent with the new text.\n",
    "    out = \"\"\n",
    "\n",
    "    links = []\n",
    "    last_index = 0\n",
    "    offset = 0\n",
    "\n",
    "    for match in re.finditer(r\"\\[\\[(.*?)(?:\\|(.*?))?\\]\\]\", text):\n",
    "        entity_name, link_text = match.groups()\n",
    "        link_text = link_text or entity_name\n",
    "\n",
    "        out += text[last_index:match.start()] + link_text\n",
    "        last_index = match.end()\n",
    "\n",
    "        # Calculate new begin after removing the link text\n",
    "        begin = match.start() - offset\n",
    "        end = begin + len(link_text)\n",
    "\n",
    "        # Update offset - for each link we remove, the text gets shorter by the length of the link text\n",
    "        offset += match.end() - match.start() - len(link_text)\n",
    "\n",
    "        if not (link_text.startswith(\"Category:\") or link_text.startswith(\"File:\")):\n",
    "            links.append({\"begin\": begin, \"end\": end, \"link\": entity_name, \"text\": link_text})\n",
    "\n",
    "    return out, links\n",
    "\n",
    "\n",
    "def parse_wikipedia_dump(dump_file: str) -> Generator[Tuple[str, str, str, List[Dict]], None, None]:\n",
    "    cleaner = wiki.cleaner.Cleaner()\n",
    "    # for id, title, text in wiki.iterate(dump_file):\n",
    "    for title, text in wiki.iterate(dump_file):\n",
    "        text = cleaner.clean_text(text)\n",
    "        # Their version is very complicated and buggy, so I just implemented it using regex\n",
    "        # cleaned_text, links = cleaner.build_links(text)\n",
    "        cleaned_text, links = my_build_links(text)\n",
    "\n",
    "        assert id is not None, f\"ID is None for title: {title}\"\n",
    "        assert title is not None, f\"Title is None for ID: {id}\"\n",
    "\n",
    "        yield id, title, cleaned_text, links\n",
    "\n",
    "def parse_link(link: dict) -> Tuple[int, int, str, str]:\n",
    "    return int(link['begin']), int(link['end']), link['link'], link['text']\n",
    "\n",
    "def is_coref_cluster_linked(link_start: int, link_end: int, coref_entity_cluster: Tuple[Tuple[int, int],...]) -> bool:\n",
    "    for entity_start, entity_end in coref_entity_cluster:\n",
    "        if link_start >= entity_start and link_end <= entity_end:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def get_coref_clusters(coref: FCoref, texts: List[str], as_strings=False) -> List[List[Tuple[Tuple[int, int],...]]]:\n",
    "    return [x.get_clusters(as_strings=as_strings) for x in coref.predict(texts, max_tokens_in_batch=1000000)]\n",
    "\n",
    "def get_all_linked_entities(coref_clusters: List[Tuple[Tuple[int, int],...]], links: List[Dict]) -> Generator[Tuple[int, int, str], None, None]:\n",
    "    for link in links:\n",
    "        found = False\n",
    "        link_start, link_end, entity_name, link_text = parse_link(link)\n",
    "        for entity_cluster in coref_clusters:\n",
    "            if is_coref_cluster_linked(link_start, link_end, entity_cluster):\n",
    "                # We have a link that's contained within a span of a coref cluster, meaning the whole cluster is related to that link\n",
    "                for entity_start, entity_end in entity_cluster:\n",
    "                    yield (entity_start, entity_end, entity_name)\n",
    "\n",
    "                # Remove the cluster from the list so we don't double count\n",
    "                coref_clusters.remove(entity_cluster)\n",
    "\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "        if not found:\n",
    "            yield (link_start, link_end, entity_name)\n",
    "\n",
    "\n",
    "def parse(dump_file: str) -> Generator[Tuple[str, str, int, int, str], None, None]:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    coref = FCoref(device=device, enable_progress_bar=False)\n",
    "    for id, title, text, links in parse_wikipedia_dump(dump_file):\n",
    "        coref_clusters = get_coref_clusters(coref, [text])[0]\n",
    "        for entity_start, entity_end, entity_name in get_all_linked_entities(coref_clusters, links):\n",
    "            yield id, title, entity_start, entity_end, entity_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_file = \"/home/morg/students/ohavbarbi/knowledge_analysis_suite/data/wikidatawiki-latest-pages-articles1.xml-p1p441397\"\n",
    "g = parse(wiki_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/25/2024 12:49:41 - INFO - \t missing_keys: []\n",
      "09/25/2024 12:49:41 - INFO - \t unexpected_keys: []\n",
      "09/25/2024 12:49:41 - INFO - \t mismatched_keys: []\n",
      "09/25/2024 12:49:41 - INFO - \t error_msgs: []\n",
      "09/25/2024 12:49:41 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n",
      "09/25/2024 12:49:41 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.50 examples/s]\n",
      "09/25/2024 12:49:41 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:47 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 272.75 examples/s]\n",
      "09/25/2024 12:49:47 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:47 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 277.97 examples/s]\n",
      "09/25/2024 12:49:47 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:47 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 281.08 examples/s]\n",
      "09/25/2024 12:49:47 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:47 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 259.63 examples/s]\n",
      "09/25/2024 12:49:47 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:48 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 281.27 examples/s]\n",
      "09/25/2024 12:49:48 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:48 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 280.52 examples/s]\n",
      "09/25/2024 12:49:48 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:48 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 280.80 examples/s]\n",
      "09/25/2024 12:49:48 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:48 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 279.58 examples/s]\n",
      "09/25/2024 12:49:48 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:48 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 280.18 examples/s]\n",
      "09/25/2024 12:49:48 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:48 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 276.94 examples/s]\n",
      "09/25/2024 12:49:49 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:49 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 281.03 examples/s]\n",
      "09/25/2024 12:49:49 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:49 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 279.97 examples/s]\n",
      "09/25/2024 12:49:49 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:49 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 281.16 examples/s]\n",
      "09/25/2024 12:49:49 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:49 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 279.01 examples/s]\n",
      "09/25/2024 12:49:49 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:50 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 278.88 examples/s]\n",
      "09/25/2024 12:49:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:50 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 278.51 examples/s]\n",
      "09/25/2024 12:49:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:50 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 281.10 examples/s]\n",
      "09/25/2024 12:49:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:50 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 281.18 examples/s]\n",
      "09/25/2024 12:49:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:50 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 280.42 examples/s]\n",
      "09/25/2024 12:49:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:50 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 272.64 examples/s]\n",
      "09/25/2024 12:49:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:50 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 278.91 examples/s]\n",
      "09/25/2024 12:49:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:50 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 273.57 examples/s]\n",
      "09/25/2024 12:49:50 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:50 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 281.33 examples/s]\n",
      "09/25/2024 12:49:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:51 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 258.97 examples/s]\n",
      "09/25/2024 12:49:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:51 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 276.78 examples/s]\n",
      "09/25/2024 12:49:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:51 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 275.72 examples/s]\n",
      "09/25/2024 12:49:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:51 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 274.95 examples/s]\n",
      "09/25/2024 12:49:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:51 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 260.03 examples/s]\n",
      "09/25/2024 12:49:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:51 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 277.44 examples/s]\n",
      "09/25/2024 12:49:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:51 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 279.25 examples/s]\n",
      "09/25/2024 12:49:51 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "09/25/2024 12:49:51 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.85 examples/s]\n",
      "09/25/2024 12:49:52 - INFO - \t ***** Running Inference on 1 texts *****\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.33 GiB. GPU 0 has a total capacity of 11.90 GiB of which 1.41 GiB is free. Including non-PyTorch memory, this process has 7.00 GiB memory in use. Process 381940 has 3.49 GiB memory in use. Of the allocated memory 5.91 GiB is allocated by PyTorch, and 530.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 87\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(dump_file)\u001b[0m\n\u001b[1;32m     85\u001b[0m coref \u001b[38;5;241m=\u001b[39m FCoref(device\u001b[38;5;241m=\u001b[39mdevice, enable_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, title, text, links \u001b[38;5;129;01min\u001b[39;00m parse_wikipedia_dump(dump_file):\n\u001b[0;32m---> 87\u001b[0m     coref_clusters \u001b[38;5;241m=\u001b[39m \u001b[43mget_coref_clusters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entity_start, entity_end, entity_name \u001b[38;5;129;01min\u001b[39;00m get_all_linked_entities(coref_clusters, links):\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mid\u001b[39m, title, entity_start, entity_end, entity_name\n",
      "Cell \u001b[0;32mIn[16], line 61\u001b[0m, in \u001b[0;36mget_coref_clusters\u001b[0;34m(coref, texts, as_strings)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_coref_clusters\u001b[39m(coref: FCoref, texts: List[\u001b[38;5;28mstr\u001b[39m], as_strings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[Tuple[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [x\u001b[38;5;241m.\u001b[39mget_clusters(as_strings\u001b[38;5;241m=\u001b[39mas_strings) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcoref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000000\u001b[39;49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m/home/morg/students/ohavbarbi/miniforge3/envs/knowledge/lib/python3.12/site-packages/fastcoref/modeling.py:265\u001b[0m, in \u001b[0;36mCorefModel.predict\u001b[0;34m(self, texts, is_split_into_words, max_tokens_in_batch, output_file)\u001b[0m\n\u001b[1;32m    262\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataset(texts, is_split_into_words)\n\u001b[1;32m    263\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_batches(dataset, max_tokens_in_batch)\n\u001b[0;32m--> 265\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/home/morg/students/ohavbarbi/miniforge3/envs/knowledge/lib/python3.12/site-packages/fastcoref/modeling.py:204\u001b[0m, in \u001b[0;36mCorefModel._inference\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m--> 204\u001b[0m         results\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(results, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m res: res\u001b[38;5;241m.\u001b[39mtext_idx)\n",
      "File \u001b[0;32m/home/morg/students/ohavbarbi/miniforge3/envs/knowledge/lib/python3.12/site-packages/fastcoref/modeling.py:165\u001b[0m, in \u001b[0;36mCorefModel._batch_inference\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    163\u001b[0m idxs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 165\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_all_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m outputs_np \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(tensor\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m outputs)\n\u001b[1;32m    169\u001b[0m span_starts, span_ends, mention_logits, coref_logits \u001b[38;5;241m=\u001b[39m outputs_np\n",
      "File \u001b[0;32m/home/morg/students/ohavbarbi/miniforge3/envs/knowledge/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/morg/students/ohavbarbi/miniforge3/envs/knowledge/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/morg/students/ohavbarbi/miniforge3/envs/knowledge/lib/python3.12/site-packages/fastcoref/coref_models/modeling_fcoref.py:254\u001b[0m, in \u001b[0;36mFCorefModel.forward\u001b[0;34m(self, batch, gold_clusters, topk_1d_indices, return_all_outputs)\u001b[0m\n\u001b[1;32m    251\u001b[0m end_coref_reps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_coref_mlp(sequence_output)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# mention scores\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m mention_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calc_mention_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_mention_reps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_mention_reps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# prune mentions\u001b[39;00m\n\u001b[1;32m    257\u001b[0m mention_start_ids, mention_end_ids, span_mask, topk_mention_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prune_topk_mentions(mention_logits, attention_mask, topk_1d_indices)\n",
      "File \u001b[0;32m/home/morg/students/ohavbarbi/miniforge3/envs/knowledge/lib/python3.12/site-packages/fastcoref/coref_models/modeling_fcoref.py:190\u001b[0m, in \u001b[0;36mFCorefModel._calc_mention_logits\u001b[0;34m(self, start_mention_reps, end_mention_reps)\u001b[0m\n\u001b[1;32m    186\u001b[0m temp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmention_s2e_classifier(start_mention_reps)  \u001b[38;5;66;03m# [batch_size, seq_length]\u001b[39;00m\n\u001b[1;32m    187\u001b[0m joint_mention_logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(temp,\n\u001b[1;32m    188\u001b[0m                                     end_mention_reps\u001b[38;5;241m.\u001b[39mpermute([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m]))  \u001b[38;5;66;03m# [batch_size, seq_length, seq_length]\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m mention_logits \u001b[38;5;241m=\u001b[39m \u001b[43mjoint_mention_logits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart_mention_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mend_mention_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m mention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_mention_mask(mention_logits)  \u001b[38;5;66;03m# [batch_size, seq_length, seq_length]\u001b[39;00m\n\u001b[1;32m    192\u001b[0m mention_logits \u001b[38;5;241m=\u001b[39m mask_tensor(mention_logits, mention_mask)  \u001b[38;5;66;03m# [batch_size, seq_length, seq_length]\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.33 GiB. GPU 0 has a total capacity of 11.90 GiB of which 1.41 GiB is free. Including non-PyTorch memory, this process has 7.00 GiB memory in use. Process 381940 has 3.49 GiB memory in use. Of the allocated memory 5.91 GiB is allocated by PyTorch, and 530.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "gen = parse(wiki_file)\n",
    "for value in gen:\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in wiki.iterate(wiki_file):\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowledge",
   "language": "python",
   "name": "knowledge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
