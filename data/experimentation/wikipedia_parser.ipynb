{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is just a copy of the `.py` file so it's easier to run remotely**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from fastcoref import FCoref\n",
    "import wiki_dump_reader as wiki\n",
    "from json_writer import JsonWriter\n",
    "from typing import Dict, Generator, List, Tuple\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "REDIRECT_REGEX = re.compile('^REDIRECT \\\\[\\\\[([a-zA-Z ]+)\\\\]\\\\]$')\n",
    "\n",
    "def my_build_links(text: str) -> Tuple[str, List[Dict]]:\n",
    "    # We don't want to just remove the links, we want to replace them with the link text so that the returned text\n",
    "    # is parseable. We also want to make sure the links indices are consistent with the new text.\n",
    "    out = \"\"\n",
    "\n",
    "    links = []\n",
    "    last_index = 0\n",
    "    offset = 0\n",
    "\n",
    "    for match in re.finditer(r\"\\[\\[(.*?)(?:\\|(.*?))?\\]\\]\", text):\n",
    "        entity_name, link_text = match.groups()\n",
    "        link_text = link_text or entity_name\n",
    "\n",
    "        out += text[last_index:match.start()] + link_text\n",
    "        last_index = match.end()\n",
    "\n",
    "        # Calculate new begin after removing the link text\n",
    "        begin = match.start() - offset\n",
    "        end = begin + len(link_text)\n",
    "\n",
    "        # Update offset - for each link we remove, the text gets shorter by the length of the link text\n",
    "        offset += match.end() - match.start() - len(link_text)\n",
    "\n",
    "        if not (entity_name.startswith(\"Category:\") or entity_name.startswith(\"File:\")):\n",
    "            links.append({\"begin\": begin, \"end\": end, \"link\": entity_name, \"text\": link_text})\n",
    "\n",
    "    return out, links\n",
    "\n",
    "\n",
    "def parse_wikipedia_dump(dump_file: str) -> Generator[Tuple[str, str, str, List[Dict], bool], None, None]:\n",
    "    cleaner = wiki.cleaner.Cleaner()\n",
    "    for id, title, text in wiki.iterate(dump_file):\n",
    "        text = cleaner.clean_text(text)\n",
    "        assert id is not None, f\"ID is None for title: {title}\"\n",
    "        assert title is not None, f\"Title is None for ID: {id}\"\n",
    "        \n",
    "        # Check for redirection page\n",
    "        match = REDIRECT_REGEX.match(text)\n",
    "        if match is not None:\n",
    "            # Is a redirect\n",
    "            redirect_name = match.group(1)\n",
    "            yield id, title, redirect_name, None, True\n",
    "        else:\n",
    "            # Is a normal page\n",
    "            # Their version is very complicated and buggy, so I just implemented it using regex\n",
    "            # cleaned_text, links = cleaner.build_links(text)\n",
    "            cleaned_text, links = my_build_links(text)\n",
    "            yield id, title, cleaned_text, links, False\n",
    "\n",
    "def parse_link(link: dict) -> Tuple[int, int, str, str]:\n",
    "    return int(link['begin']), int(link['end']), link['link'], link['text']\n",
    "\n",
    "def is_coref_cluster_linked(link_start: int, link_end: int, coref_entity_cluster: Tuple[Tuple[int, int],...]) -> bool:\n",
    "    for entity_start, entity_end in coref_entity_cluster:\n",
    "        if link_start >= entity_start and link_end <= entity_end:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def get_coref_clusters(coref: FCoref, texts: List[str], as_strings=False) -> List[List[Tuple[Tuple[int, int],...]]]:\n",
    "    return [x.get_clusters(as_strings=as_strings) for x in coref.predict(texts, max_tokens_in_batch=100000)]\n",
    "\n",
    "def get_all_linked_entities(coref_clusters: List[Tuple[Tuple[int, int],...]], links: List[Dict]) -> Generator[Tuple[int, int, str], None, None]:\n",
    "    for link in links:\n",
    "        found = False\n",
    "        link_start, link_end, entity_name, link_text = parse_link(link)\n",
    "        for entity_cluster in coref_clusters:\n",
    "            if is_coref_cluster_linked(link_start, link_end, entity_cluster):\n",
    "                # We have a link that's contained within a span of a coref cluster, meaning the whole cluster is related to that link\n",
    "                for entity_start, entity_end in entity_cluster:\n",
    "                    yield (entity_start, entity_end, entity_name)\n",
    "\n",
    "                # Remove the cluster from the list so we don't double count\n",
    "                coref_clusters.remove(entity_cluster)\n",
    "\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "        if not found:\n",
    "            yield (link_start, link_end, entity_name)\n",
    "\n",
    "\n",
    "def parse(dump_file_path: str, device: str) -> Generator[Tuple[str, str, int, int, str], None, None]:\n",
    "    coref = FCoref(device=device, enable_progress_bar=False)\n",
    "    for id, title, text, links, is_redirect in parse_wikipedia_dump(dump_file_path):\n",
    "        if is_redirect:\n",
    "            yield id, title, text, links, is_redirect \n",
    "        else:\n",
    "            coref_clusters = get_coref_clusters(coref, [text])[0]\n",
    "            for entity_start, entity_end, entity_name in get_all_linked_entities(coref_clusters, links):\n",
    "                yield id, title, entity_start, entity_end, entity_name\n",
    "\n",
    "def write(dump_file_path: str, out_base_path: str, device: str):\n",
    "    coref = FCoref(device=device, enable_progress_bar=True)\n",
    "    writer = JsonWriter(out_base_path, verbose=True)\n",
    "    for id, title, text, links, is_redirect in parse_wikipedia_dump(dump_file_path):\n",
    "        ents = []\n",
    "        if not is_redirect:\n",
    "            # Only normal pages have entities.\n",
    "            coref_clusters = get_coref_clusters(coref, [text])[0]\n",
    "            ents = []\n",
    "            for entity_start, entity_end, entity_name in get_all_linked_entities(coref_clusters, links):\n",
    "                ents.append({\"entity_start\": entity_start, \"entity_end\": entity_end, \"entity_name\": entity_name})\n",
    "            del coref_clusters    \n",
    "            torch.cuda.empty_cache()\n",
    "        writer.write({\"src_file\": dump_file_path, \"id\": id, \"text\": text, \"title\": title, \"entities\": ents, 'is_redirect': is_redirect})  \n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 AccessibleComputing Computer accessibility None True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from fastcoref import FCoref\n",
    "import wiki_dump_reader as wiki\n",
    "from typing import Dict, Generator, List, Tuple\n",
    "\n",
    "dump_file = '/home/morg/dataset/enwiki/enwiki-latest-pages-articles1.xml-p1p41242'\n",
    "cleaner = wiki.cleaner.Cleaner()\n",
    "for id, title, text in wiki.iterate(dump_file):\n",
    "    text = cleaner.clean_text(text)\n",
    "    assert id is not None, f\"ID is None for title: {title}\"\n",
    "    assert title is not None, f\"Title is None for ID: {id}\"\n",
    "    \n",
    "    # Check for redirection page\n",
    "    match = REDIRECT_REGEX.match(text)\n",
    "    if match is not None:\n",
    "        # Is a redirect\n",
    "        redirect_name = match.group(1)\n",
    "        print(id, title, redirect_name, None, True)\n",
    "    else:\n",
    "        # Is a normal page\n",
    "        # Their version is very complicated and buggy, so I just implemented it using regex\n",
    "        # cleaned_text, links = cleaner.build_links(text)\n",
    "        cleaned_text, links = my_build_links(text)\n",
    "        print(id, title, cleaned_text, links, False)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowledge",
   "language": "python",
   "name": "knowledge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
